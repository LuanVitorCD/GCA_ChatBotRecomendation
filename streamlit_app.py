# streamlit_app.py - Interface Final com Piv√¥ para Motor Legado, LLM e Sync de Dados
import streamlit as st
import pandas as pd
import traceback
import time
import requests
import json
import random
import chromadb

# L√≥gica de recomenda√ß√£o
from recommend_legacy import recommend_legacy_clustering
from db_utils import get_publications_by_professor_id

# Utilit√°rios do ChromaDB
from chroma_utils import sync_postgres_to_chroma

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="RecomendaProf - Tese", layout="wide", initial_sidebar_state="expanded")



# Fun√ß√£o para aplicar tema customizado CSS
def set_custom_theme():
    st.markdown("""
        <style>
            /* Cores Gerais */
            .stApp, .stMarkdown, label, p, span, h1, h2, h3, h4, h5, h6 { 
                color: #E0E0E0 !important; 
            }
            
            /* Bot√µes Prim√°rios (Favoritar/A√ß√£o) */
            button[kind="primary"] {
                background-color: #4b67ff !important;
                color: white !important;
                border: none;
                transition: 0.3s;
            }
            button[kind="primary"]:hover {
                background-color: #3b55cc !important;
                box-shadow: 0 0 10px rgba(75, 103, 255, 0.5);
            }

            /* Alinhamento de Bot√µes na Coluna Direita */
            div[data-testid="column"] button {
                width: 100%;
                margin-bottom: 5px;
                display: flex;
                justify-content: center;
                align-items: center;
            }
            
            /* Ajuste sutil para separar o conte√∫do */
            hr { margin: 1.5em 0; border-color: #333; }
        </style>
    """, unsafe_allow_html=True)

set_custom_theme()



# Setup para sincroniza√ß√£o com ChromaDB
@st.cache_resource
def get_chroma_collection():
    """ Inicializa o cliente para permitir a sincroniza√ß√£o dos dados """
    try:
        client = chromadb.PersistentClient(path="chroma_db_cache")
        collection = client.get_or_create_collection(name="orientadores_academicos")
        return collection
    except Exception as e:
        print(f"Aviso: ChromaDB n√£o inicializado (apenas necess√°rio para sync): {e}")
        return None

collection = get_chroma_collection()



# Gerenciamento de estados
if 'favorites' not in st.session_state:
    st.session_state.favorites = {} 
if 'blacklist' not in st.session_state:
    st.session_state.blacklist = {} 
if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'current_results' not in st.session_state:
    st.session_state.current_results = []
if 'refined_query' not in st.session_state:
    st.session_state.refined_query = ""
if 'view_mode' not in st.session_state:
    st.session_state.view_mode = "search" 
if 'selected_prof' not in st.session_state:
    st.session_state.selected_prof = None



# Integra√ß√£o com LLMs
def call_ollama(prompt, model="mistral"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model, "prompt": prompt, "stream": False,
        "options": {"temperature": 0.7} 
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json().get("response", "")
    except Exception as e:
        return f"Erro ao conectar com Ollama: {e}"

def call_gemini(prompt, api_key):
    return "Integra√ß√£o Gemini ainda n√£o configurada."

def llm_refine_query(user_text, provider, model_name, api_key=None):
    system_prompt = (
        f"Atue como um assistente acad√™mico especialista. "
        f"O usu√°rio vai descrever um tema de pesquisa. "
        f"Converta isso para 3 a 5 palavras-chave t√©cnicas acad√™micas (Lattes). "
        f"Retorne APENAS as palavras separadas por espa√ßo.\n\n"
        f"Texto: '{user_text}'"
    )
    
    if provider == "Simula√ß√£o":
        refinement = user_text
        keywords = ["pesquisa", "desenvolvimento", "tecnologia", "an√°lise", "estudo"]
        if len(user_text.split()) < 4:
            refinement += " " + " ".join(keywords[:2])
        return refinement
        
    elif provider == "Local (Ollama)":
        return call_ollama(system_prompt, model=model_name)
    elif provider == "Gemini (API)":
        return call_gemini(system_prompt, api_key)
    return user_text

def llm_explain_recommendation(prof_name, score, user_query, provider, model_name, api_key=None):
    prompt = (
        f"Escreva uma justificativa curta (m√°ximo 2 frases) explicando por que o professor '{prof_name}' "
        f"√© uma boa recomenda√ß√£o para o tema '{user_query}'. "
        f"O algoritmo deu um score de {score:.2f}. Seja profissional."
    )
    
    if provider == "Simula√ß√£o":
        templates = [
            f"Com base na busca por '{user_query}', o algoritmo identificou **{prof_name}** como forte correspond√™ncia (Score: {score:.2f}).",
            f"A trajet√≥ria acad√™mica de **{prof_name}** apresenta alta sinergia com o tema '{user_query}', refletida no √≠ndice {score:.2f}.",
            f"Para o tema '{user_query}', **{prof_name}** destaca-se pela produtividade e experi√™ncia na √°rea (√çndice: {score:.2f})."
        ]
        random.seed(prof_name + user_query) 
        return random.choice(templates)
        
    elif provider == "Local (Ollama)":
        return call_ollama(prompt, model=model_name)
    return "Explica√ß√£o indispon√≠vel."



# Parser e l√≥gicas auxiliares
def parse_legacy_results(legacy_string):
    results = []
    if "Nenhum orientador" in legacy_string:
        return []
    lines = legacy_string.strip().split('\n\n')
    for line in lines:
        parts = line.split(' - Rating: ')
        if len(parts) == 2:
            nome = parts[0]
            try: score = float(parts[1])
            except: score = 0.0
            id_ficticio = nome.replace(" ", "_").lower()
            
            if id_ficticio not in st.session_state.blacklist:
                results.append({'nome': nome, 'hybrid_score': score, 'id': id_ficticio})
    return results

def toggle_favorite(prof):
    prof_id = prof['id']
    if prof_id in st.session_state.favorites:
        del st.session_state.favorites[prof_id]
        st.toast(f"Removido dos favoritos.", icon="üóëÔ∏è")
    else:
        if prof_id in st.session_state.blacklist:
            del st.session_state.blacklist[prof_id]
        st.session_state.favorites[prof_id] = prof 
        st.toast(f"Favoritado!", icon="‚≠ê")

def toggle_blacklist(prof):
    prof_id = prof['id']
    if prof_id in st.session_state.blacklist:
        del st.session_state.blacklist[prof_id]
        st.toast(f"Restaurado.", icon="üëÅÔ∏è")
    else:
        if prof_id in st.session_state.favorites:
            del st.session_state.favorites[prof_id]
        st.session_state.blacklist[prof_id] = prof
        st.toast(f"Ocultado.", icon="üö´")
        st.session_state.current_results = [p for p in st.session_state.current_results if p['id'] != prof_id]

def clear_search():
    st.session_state.current_results = []
    st.session_state.refined_query = ""
    st.session_state.view_mode = "search"
    st.rerun()

def view_professor_details(prof):
    st.session_state.selected_prof = prof
    st.session_state.view_mode = "single_view"
    st.rerun()

def back_to_search():
    st.session_state.view_mode = "search"
    st.session_state.selected_prof = None
    st.rerun()



# INTERFACE LATERAL #
with st.sidebar:
    st.title("üéì RecomendaProf")
    st.caption("Baseado na Tese de Doutorado de Radi Melo Martins")
    
    st.divider()
    st.subheader("üß† Configura√ß√£o da IA")
    llm_provider = st.selectbox("Provedor de Intelig√™ncia:", ["Simula√ß√£o", "Local (Ollama)", "Gemini (API)"])
    
    ollama_model = "mistral"
    api_key = None
    if llm_provider == "Local (Ollama)":
        ollama_model = st.text_input("Modelo Ollama:", value="mistral")
    elif llm_provider == "Gemini (API)":
        api_key = st.text_input("API Key do Google:", type="password")

    st.divider()
    st.subheader("Filtros & Limites")
    only_doctors = st.checkbox("Apenas Doutorado", value=True)
    max_professors = st.slider("M√°x. Professores", 1, 20, 5)
    max_pubs_limit = st.slider("M√°x. Publica√ß√µes", 1, 10, 3)

    st.divider()
    
    st.subheader(f"‚≠ê Favoritos ({len(st.session_state.favorites)})")
    if st.session_state.favorites:
        for fav_id, fav_data in list(st.session_state.favorites.items()):
            c1, c2 = st.columns([4, 1])
            if c1.button(fav_data['nome'], key=f"nav_fav_{fav_id}"):
                view_professor_details(fav_data)
            if c2.button("‚úï", key=f"rm_fav_{fav_id}"):
                 del st.session_state.favorites[fav_id]
                 st.rerun()
    else:
        st.caption("Nenhum favorito ainda.")

    st.divider()
    if st.session_state.blacklist:
        with st.expander(f"üö´ Ocultados ({len(st.session_state.blacklist)})"):
             for black_id, black_data in list(st.session_state.blacklist.items()):
                c1, c2 = st.columns([4, 1])
                c1.text(black_data['nome'])
                if c2.button("‚Ü∫", key=f"rst_{black_id}"):
                    del st.session_state.blacklist[black_id]
                    st.rerun()

    # Bot√£o para sincronizar dados entre BDs
    st.divider()
    st.markdown("### üîÑ Dados")
    st.caption("Sincroniza√ß√£o PostgreSQL -> ChromaDB (Opcional)")
    if st.button("Sincronizar Banco", use_container_width=True):
        if collection is None:
            st.error("ChromaDB n√£o inicializado.")
        else:
            try:
                with st.spinner("Lendo do PostgreSQL e vetorizando..."):
                    count = sync_postgres_to_chroma(collection)
                st.success(f"{count} perfis sincronizados com sucesso!")
            except Exception as e:
                st.error("Falha na sincroniza√ß√£o.")
                st.code(str(e))



# INTERFACE PRINCIPAL #
st.title("Encontre seu Orientador Ideal")

# Detalhes
if st.session_state.view_mode == "single_view" and st.session_state.selected_prof:
    prof = st.session_state.selected_prof
    
    if st.button("‚Üê Voltar para a busca"):
        back_to_search()
        
    with st.container(border=True):
        st.header(prof['nome'])
        st.caption(f"√çndice de Recomenda√ß√£o: **{prof['hybrid_score']:.2f}**")
        
        query_context = st.session_state.refined_query if st.session_state.refined_query else "sua pesquisa"
        explanation = llm_explain_recommendation(prof['nome'], prof['hybrid_score'], query_context, llm_provider, ollama_model, api_key)
        st.info(explanation)
        
        st.subheader("Publica√ß√µes Detalhadas")
        pubs, total = get_publications_by_professor_id(prof['id'], limit=10)
        if pubs:
            st.write(f"Mostrando as 10 mais recentes de {total} encontradas:")
            for p in pubs: st.markdown(f"- {p}")
        else:
            st.warning("Nenhuma publica√ß√£o encontrada no banco de dados.")
            
    c1, c2 = st.columns(2)
    is_fav = prof['id'] in st.session_state.favorites
    if c1.button("‚òÖ Remover Favorito" if is_fav else "‚òÜ Favoritar", key="det_fav", use_container_width=True, type="primary" if is_fav else "secondary"):
        toggle_favorite(prof)
        st.rerun()
    if c2.button("üö´ Ocultar Professor", key="det_blk", use_container_width=True):
        toggle_blacklist(prof)
        back_to_search()



# Modo de busca padr√£o
else:
    if st.session_state.search_history:
        with st.expander("Ver hist√≥rico da conversa", expanded=False):
            for msg in st.session_state.search_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

    if prompt := st.chat_input("Ex: Quero pesquisar sobre uso de machine learning na detec√ß√£o de c√¢ncer..."):
        st.session_state.search_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)

        with st.status("üß† Processando...", expanded=True) as status:
            st.write(f"Refinando busca com {llm_provider}...")
            refined_query = llm_refine_query(prompt, llm_provider, ollama_model, api_key)
            st.session_state.refined_query = refined_query
            st.write(f"üîç Termos gerados: *'{refined_query}'*")
            
            try:
                legacy_raw = recommend_legacy_clustering(refined_query, only_doctors)
                parsed_results = parse_legacy_results(legacy_raw)
                st.session_state.current_results = parsed_results
                status.update(label="Busca conclu√≠da!", state="complete", expanded=False)
            except Exception as e:
                status.update(label="Erro na busca", state="error")
                st.error(f"Erro no motor legado: {e}")
                st.session_state.current_results = []

    if st.session_state.current_results:
        col_res_1, col_res_2 = st.columns([5, 1])
        results_to_show = st.session_state.current_results[:max_professors]
        col_res_1.subheader(f"Resultados ({len(results_to_show)})")
        if col_res_2.button("Limpar Busca", type="secondary"):
            clear_search()

        st.markdown(f"Baseado nos termos: *{st.session_state.refined_query}*")
        
        for prof in results_to_show:
            is_fav = prof['id'] in st.session_state.favorites
            fav_label = "‚òÖ Favorito" if is_fav else "‚òÜ Favoritar"
            fav_type = "primary" if is_fav else "secondary"

            with st.container(border=True):
                c1, c2 = st.columns([3, 1])
                
                with c1:
                    st.markdown(f"### {prof['nome']}")
                    st.caption(f"√çndice de Recomenda√ß√£o: **{prof['hybrid_score']:.2f}**")
                    
                    if 'refined_query' in st.session_state:
                         explanation = llm_explain_recommendation(prof['nome'], prof['hybrid_score'], st.session_state.refined_query, llm_provider, ollama_model, api_key)
                         st.info(explanation)

                    with st.expander("Ver publica√ß√µes recentes"):
                        pubs, _ = get_publications_by_professor_id(prof['id'], limit=max_pubs_limit)
                        if pubs:
                            for p in pubs: st.text(f"‚Ä¢ {p}")
                        else:
                            st.caption("Nenhuma publica√ß√£o encontrada.")

                with c2:
                    st.write("") 
                    if st.button(fav_label, key=f"btn_fav_{prof['id']}", type=fav_type, use_container_width=True):
                        toggle_favorite(prof)
                        st.rerun()
                    
                    if st.button("üö´ Ocultar", key=f"btn_blk_{prof['id']}", use_container_width=True):
                        toggle_blacklist(prof)
                        st.rerun()
                    
                    if st.button("üìÑ Detalhes", key=f"btn_det_{prof['id']}", use_container_width=True):
                        view_professor_details(prof)

    elif st.session_state.search_history and not st.session_state.current_results:
         st.info("Fa√ßa uma nova busca para ver resultados.")